<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="MÃ³dulo 3: LangChain - Domine o framework lÃ­der para desenvolvimento de aplicaÃ§Ãµes IA com RAG, Agentes e MemÃ³ria">
    <title>MÃ³dulo 3: Frameworks - LangChain | FEA-IA 2.0</title>

    <link rel="icon" href="../assets/favicon.svg" type="image/svg+xml">
    <link rel="stylesheet" href="../css/styles.css">
    <link rel="stylesheet" href="../css/topics.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet">
    <script defer src="../js/app.js"></script>

    <style>
        .module-hero {
            padding: 6rem 0 4rem;
            background: linear-gradient(135deg, #8b5cf6 0%, #6d28d9 100%);
            color: white;
        }

        .module-breadcrumb {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            font-size: 0.875rem;
            margin-bottom: 1rem;
            opacity: 0.9;
        }

        .module-breadcrumb a {
            color: white;
            text-decoration: none;
        }

        .module-breadcrumb a:hover {
            text-decoration: underline;
        }

        .module-content-area {
            padding: 4rem 0;
            max-width: 900px;
            margin: 0 auto;
        }

        .module-header-info {
            display: flex;
            gap: 1.5rem;
            margin-bottom: 2rem;
            flex-wrap: wrap;
        }

        .info-badge {
            background: rgba(255, 255, 255, 0.2);
            padding: 0.5rem 1rem;
            border-radius: 8px;
            font-size: 0.875rem;
            backdrop-filter: blur(10px);
        }

        .module-quote {
            font-size: 1.25rem;
            font-style: italic;
            opacity: 0.95;
            margin: 2rem 0;
            padding-left: 1.5rem;
            border-left: 4px solid rgba(255, 255, 255, 0.5);
        }

        /* Content Sections */
        .content-section {
            margin-bottom: 3rem;
        }

        .content-section h2 {
            color: #8b5cf6;
            margin-bottom: 1rem;
            font-size: 2rem;
            font-weight: 700;
        }

        .content-section h3 {
            color: #6d28d9;
            margin: 2rem 0 1rem;
            font-size: 1.5rem;
            font-weight: 600;
        }

        .content-section h4 {
            color: #333;
            margin: 1.5rem 0 0.75rem;
            font-size: 1.25rem;
            font-weight: 600;
        }

        .content-section p {
            line-height: 1.8;
            color: #374151;
            margin-bottom: 1rem;
        }

        .content-section ul, .content-section ol {
            margin: 1rem 0;
            padding-left: 2rem;
        }

        .content-section li {
            margin: 0.5rem 0;
            line-height: 1.8;
            color: #374151;
        }

        /* Code blocks */
        pre {
            background: #1e293b;
            color: #e2e8f0;
            padding: 1.5rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1.5rem 0;
            font-family: 'Courier New', monospace;
            font-size: 0.875rem;
            line-height: 1.6;
        }

        code {
            background: #f1f5f9;
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            font-size: 0.875rem;
            color: #8b5cf6;
        }

        pre code {
            background: transparent;
            padding: 0;
            color: #e2e8f0;
        }

        /* Diagrams & ASCII art */
        .diagram {
            background: #f8fafc;
            padding: 1.5rem;
            border-radius: 8px;
            border: 2px solid #e2e8f0;
            margin: 1.5rem 0;
            font-family: 'Courier New', monospace;
            font-size: 0.875rem;
            overflow-x: auto;
            white-space: pre;
        }

        /* Info boxes */
        .info-box {
            background: #eff6ff;
            border-left: 4px solid #3b82f6;
            padding: 1rem 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }

        .warning-box {
            background: #fef3c7;
            border-left: 4px solid #f59e0b;
            padding: 1rem 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }

        .success-box {
            background: #d1fae5;
            border-left: 4px solid #10b981;
            padding: 1rem 1.5rem;
            margin: 1.5rem 0;
            border-radius: 4px;
        }

        /* Project section */
        .project-section {
            background: linear-gradient(135deg, #f0fdf4 0%, #dcfce7 100%);
            padding: 2rem;
            border-radius: 12px;
            margin: 3rem 0;
        }

        .project-section h2 {
            color: #059669;
        }

        /* Navigation buttons */
        .module-navigation {
            display: flex;
            justify-content: space-between;
            margin-top: 4rem;
            padding-top: 2rem;
            border-top: 2px solid #e5e7eb;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .nav-button {
            flex: 1;
            min-width: 200px;
            padding: 1rem 1.5rem;
            border-radius: 8px;
            text-decoration: none;
            font-weight: 600;
            transition: all 0.3s ease;
            text-align: center;
        }

        .nav-button.prev {
            background: #f3f4f6;
            color: #374151;
        }

        .nav-button.next {
            background: linear-gradient(135deg, #8b5cf6 0%, #6d28d9 100%);
            color: white;
        }

        .nav-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
        }

        /* Responsive */
        @media (max-width: 768px) {
            .module-hero {
                padding: 4rem 0 2rem;
            }

            .module-content-area {
                padding: 2rem 0;
            }

            .module-quote {
                font-size: 1rem;
            }

            .content-section h2 {
                font-size: 1.5rem;
            }

            .content-section h3 {
                font-size: 1.25rem;
            }

            pre {
                padding: 1rem;
                font-size: 0.75rem;
            }
        }
    </style>
</head>
<body>
    <!-- Hero Section -->
    <section class="module-hero">
        <div class="container">
            <div class="module-breadcrumb">
                <a href="../index.html">InÃ­cio</a>
                <span>â†’</span>
                <span>MÃ³dulo 3</span>
            </div>

            <h1 class="display-1">MÃ³dulo 3: Frameworks - LangChain</h1>

            <div class="module-header-info">
                <div class="info-badge">â±ï¸ 10 horas</div>
                <div class="info-badge">ğŸ“Š NÃ­vel: IntermediÃ¡rio</div>
                <div class="info-badge">ğŸ¯ NÃ­vel: AplicaÃ§Ã£o</div>
                <div class="info-badge">ğŸ“„ 1.801 linhas</div>
            </div>

            <p class="module-quote">
                "NÃ£o construa do zero. Orquestre o que jÃ¡ existe."
            </p>
        </div>
    </section>

    <!-- Topics Compact Navigation -->
    <div class="topics-container" id="topics-container">
        <div class="topics-header">
            <h3>ğŸ“š ConteÃºdo do MÃ³dulo</h3>
            <button class="topics-toggle" id="topics-toggle" aria-label="Expandir/Recolher tÃ³picos">
                <span class="toggle-icon">â–¼</span>
            </button>
        </div>
        <div class="topics-list" id="topics-list">
            <a href="#cap-3-1" class="topic-item">
                <span class="topic-icon">ğŸ“¦</span>
                <span class="topic-text">Cap 3.1: Arquitetura LangChain</span>
            </a>
            <a href="#cap-3-2" class="topic-item">
                <span class="topic-icon">â›“ï¸</span>
                <span class="topic-text">Cap 3.2: Chains - Pipelines Inteligentes</span>
            </a>
            <a href="#cap-3-3" class="topic-item">
                <span class="topic-icon">ğŸ”</span>
                <span class="topic-text">Cap 3.3: RAG (Retrieval-Augmented Generation)</span>
            </a>
            <a href="#cap-3-4" class="topic-item">
                <span class="topic-icon">ğŸ¤–</span>
                <span class="topic-text">Cap 3.4: Agentes LangChain</span>
            </a>
            <a href="#cap-3-5" class="topic-item">
                <span class="topic-icon">ğŸ§ </span>
                <span class="topic-text">Cap 3.5: Memory - Dando MemÃ³ria aos Agentes</span>
            </a>
            <a href="#cap-3-6" class="topic-item">
                <span class="topic-icon">ğŸ”Œ</span>
                <span class="topic-text">Cap 3.6: IntegraÃ§Ãµes e Ecosystem</span>
            </a>
            <a href="#projeto" class="topic-item">
                <span class="topic-icon">ğŸš€</span>
                <span class="topic-text">Projeto PrÃ¡tico: ChatPDF Completo</span>
            </a>
            <a href="#dicas" class="topic-item">
                <span class="topic-icon">ğŸ’¡</span>
                <span class="topic-text">Dicas de OtimizaÃ§Ã£o</span>
            </a>
        </div>
    </div>

    <!-- Main Content -->
    <main class="container module-content-area">

        <!-- Introduction -->
        <section class="content-section">
            <h2>Ato 2: A ConstruÃ§Ã£o</h2>
            <p class="module-quote">
                "VocÃª aprendeu a falar com a IA. Agora, vocÃª vai dar a ela mÃ£os para agir e uma mente para lembrar. Este Ã© o momento em que passamos de meros conversadores a verdadeiros construtores de aplicaÃ§Ãµes. Com LangChain, vocÃª nÃ£o estÃ¡ mais limitado a uma Ãºnica interaÃ§Ã£o; vocÃª estÃ¡ construindo sistemas inteligentes e persistentes."
            </p>

            <p>Bem-vindo Ã  oficina do Engenheiro de Agentes. Se os LLMs sÃ£o o motor e os prompts sÃ£o o volante, <strong>LangChain Ã© o chassi, o sistema de transmissÃ£o e todo o conjunto</strong> que transforma um motor potente em um veÃ­culo funcional.</p>

            <p>Este framework de cÃ³digo aberto nos dÃ¡ os blocos de construÃ§Ã£o para criar aplicaÃ§Ãµes de IA que vÃ£o muito alÃ©m de um simples chatbot. Vamos aprender a:</p>
            <ul>
                <li><strong>Dar memÃ³ria</strong> aos nossos agentes</li>
                <li><strong>ConectÃ¡-los a fontes de dados externas</strong></li>
                <li><strong>Criar cadeias de raciocÃ­nio complexas</strong></li>
                <li><strong>Orquestrar mÃºltiplos modelos e ferramentas</strong></li>
            </ul>
        </section>

        <!-- Chapter 3.1 -->
        <section id="cap-3-1" class="content-section">
            <h2>CapÃ­tulo 3.1: A Arquitetura LangChain</h2>

            <h3>O Problema que LangChain Resolve</h3>
            <p>Imagine que vocÃª precisa construir um assistente de IA que:</p>
            <ol>
                <li>Responde perguntas sobre documentos da sua empresa</li>
                <li>Lembra conversas anteriores</li>
                <li>Pode buscar informaÃ§Ãµes na web quando necessÃ¡rio</li>
                <li>Consulta um banco de dados SQL</li>
                <li>Envia emails automaticamente</li>
            </ol>

            <p>Sem um framework, vocÃª teria que:</p>
            <ul>
                <li>Escrever cÃ³digo personalizado para cada integraÃ§Ã£o</li>
                <li>Gerenciar manualmente o estado e a memÃ³ria</li>
                <li>Implementar lÃ³gica complexa de orquestraÃ§Ã£o</li>
                <li>Lidar com diferentes formatos de API</li>
                <li>Debugar fluxos de dados entre componentes</li>
            </ul>

            <div class="success-box">
                <strong>LangChain abstrai toda essa complexidade.</strong>
            </div>

            <h3>Filosofia de Design</h3>
            <p>LangChain segue princÃ­pios fundamentais:</p>
            <ol>
                <li><strong>Modularidade:</strong> Cada componente Ã© independente e intercambiÃ¡vel</li>
                <li><strong>Composabilidade:</strong> Componentes simples se combinam para criar sistemas complexos</li>
                <li><strong>AbstraÃ§Ã£o:</strong> Interfaces unificadas para diferentes provedores e serviÃ§os</li>
                <li><strong>Extensibilidade:</strong> FÃ¡cil criar componentes customizados</li>
            </ol>

            <h3>Os 6 Pilares da Arquitetura LangChain</h3>
            <div class="diagram">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    APLICAÃ‡ÃƒO                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  Models  â”‚  â”‚ Prompts â”‚  â”‚ Chains â”‚  â”‚ Agents â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Memory  â”‚  â”‚         Tools                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</div>

            <h4>1. Models (Modelos)</h4>
            <p>Interface unificada para diferentes LLMs:</p>
            <pre><code>from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_community.llms import Ollama

# Todos usam a mesma interface!
llm_openai = ChatOpenAI(model="gpt-4")
llm_claude = ChatAnthropic(model="claude-3-sonnet")
llm_local = Ollama(model="llama3")

# Trocar de modelo = trocar 1 linha de cÃ³digo</code></pre>

            <p><strong>Tipos de modelos suportados:</strong></p>
            <ul>
                <li><strong>Chat Models:</strong> Conversacional (ChatGPT, Claude, etc.)</li>
                <li><strong>LLMs:</strong> Completion-style (GPT-3, etc.)</li>
                <li><strong>Text Embedding Models:</strong> Para criar vetores</li>
                <li><strong>Multimodal Models:</strong> VisÃ£o + Texto</li>
            </ul>

            <h4>2. Prompts</h4>
            <p>Gerenciamento avanÃ§ado de prompts:</p>
            <pre><code>from langchain.prompts import PromptTemplate, ChatPromptTemplate

# Template simples
template = PromptTemplate(
    input_variables=["produto", "publico"],
    template="""
    Crie um slogan de marketing para {produto}
    direcionado a {publico}.
    """
)

# Chat template (com papÃ©is)
chat_template = ChatPromptTemplate.from_messages([
    ("system", "VocÃª Ã© um especialista em {dominio}"),
    ("human", "{pergunta}"),
    ("ai", "{resposta_parcial}"),
    ("human", "Continue...")
])</code></pre>

            <p><strong>Recursos avanÃ§ados:</strong></p>
            <ul>
                <li>Few-shot example selectors</li>
                <li>Conditional prompting</li>
                <li>Prompt composition</li>
                <li>Versioning e A/B testing</li>
            </ul>

            <h4>3. Chains</h4>
            <p>SequÃªncias de operaÃ§Ãµes:</p>
            <pre><code>from langchain.chains import LLMChain

chain = LLMChain(
    llm=llm,
    prompt=template
)

resultado = chain.run(
    produto="TÃªnis de corrida",
    publico="atletas profissionais"
)</code></pre>

            <p><strong>Tipos de Chains:</strong></p>
            <ul>
                <li><code>LLMChain</code>: Chain bÃ¡sica (LLM + Prompt)</li>
                <li><code>SequentialChain</code>: Encadeia mÃºltiplas chains</li>
                <li><code>TransformChain</code>: Transforma dados entre chains</li>
                <li><code>RouterChain</code>: Direciona para chains especÃ­ficas</li>
                <li><code>MapReduceChain</code>: Processa documentos em paralelo</li>
            </ul>

            <h4>4. Agents</h4>
            <p>LLM como motor de decisÃ£o:</p>
            <pre><code>from langchain.agents import create_react_agent, AgentExecutor
from langchain.tools import Tool

tools = [
    Tool(
        name="Calculator",
        func=calculator_function,
        description="Ãštil para cÃ¡lculos matemÃ¡ticos"
    ),
    Tool(
        name="Search",
        func=search_function,
        description="Busca informaÃ§Ãµes na web"
    )
]

agent = create_react_agent(llm, tools, prompt)
executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

executor.invoke({"input": "Qual Ã© a raiz quadrada de 144?"})</code></pre>

            <h4>5. Memory</h4>
            <p>MemÃ³ria conversacional:</p>
            <pre><code>from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory()

# Armazena automaticamente histÃ³rico
memory.save_context(
    {"input": "Oi, meu nome Ã© JoÃ£o"},
    {"output": "OlÃ¡ JoÃ£o! Como posso ajudar?"}
)

# Recupera contexto
memory.load_memory_variables({})
# {'history': 'Human: Oi, meu nome Ã© JoÃ£o\nAI: OlÃ¡ JoÃ£o!...'}</code></pre>

            <p><strong>Tipos de Memory:</strong></p>
            <ul>
                <li><code>ConversationBufferMemory</code>: Guarda tudo</li>
                <li><code>ConversationBufferWindowMemory</code>: Ãšltimas N mensagens</li>
                <li><code>ConversationSummaryMemory</code>: Resume conversas longas</li>
                <li><code>ConversationKGMemory</code>: Extrai knowledge graph</li>
            </ul>

            <h4>6. Tools</h4>
            <p>ConexÃ£o com o mundo externo:</p>
            <pre><code>from langchain.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper

wikipedia = WikipediaQueryRun(
    api_wrapper=WikipediaAPIWrapper()
)

resultado = wikipedia.run("InteligÃªncia Artificial")</code></pre>

            <p><strong>Categorias de Tools:</strong></p>
            <ul>
                <li><strong>Search:</strong> Google, Bing, DuckDuckGo</li>
                <li><strong>APIs:</strong> Weather, News, Finance</li>
                <li><strong>Databases:</strong> SQL, MongoDB, Redis</li>
                <li><strong>Files:</strong> PDF, CSV, JSON</li>
                <li><strong>Custom:</strong> Suas prÃ³prias funÃ§Ãµes</li>
            </ul>
        </section>

        <!-- Chapter 3.2 -->
        <section id="cap-3-2" class="content-section">
            <h2>CapÃ­tulo 3.2: Chains - Construindo Pipelines Inteligentes</h2>

            <h3>LLMChain: A FundaÃ§Ã£o</h3>
            <p>A <code>LLMChain</code> Ã© o building block mais bÃ¡sico:</p>
            <pre><code>from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# 1. Definir o modelo
llm = ChatOpenAI(temperature=0.7)

# 2. Criar template de prompt
prompt = PromptTemplate(
    input_variables=["tema", "tom"],
    template="""
    Escreva um parÃ¡grafo sobre {tema}.
    Tom: {tom}
    """
)

# 3. Criar a chain
chain = LLMChain(llm=llm, prompt=prompt)

# 4. Executar
resultado = chain.invoke({
    "tema": "inteligÃªncia artificial",
    "tom": "inspirador"
})

print(resultado['text'])</code></pre>

            <h3>SequentialChain: Workflows Multi-Etapa</h3>
            <p>Crie pipelines complexos onde a saÃ­da de uma chain alimenta a prÃ³xima:</p>
            <pre><code>from langchain.chains import SequentialChain

# Chain 1: Gerar ideia de produto
chain_ideia = LLMChain(
    llm=llm,
    prompt=PromptTemplate(
        input_variables=["categoria"],
        template="Sugira um nome criativo para um produto na categoria: {categoria}"
    ),
    output_key="nome_produto"
)

# Chain 2: Criar descriÃ§Ã£o
chain_descricao = LLMChain(
    llm=llm,
    prompt=PromptTemplate(
        input_variables=["nome_produto"],
        template="Escreva uma descriÃ§Ã£o de marketing para: {nome_produto}"
    ),
    output_key="descricao"
)

# Chain 3: Gerar slogan
chain_slogan = LLMChain(
    llm=llm,
    prompt=PromptTemplate(
        input_variables=["nome_produto", "descricao"],
        template="""
        Baseado no produto {nome_produto} e sua descriÃ§Ã£o:
        {descricao}

        Crie um slogan memorÃ¡vel (mÃ¡ximo 7 palavras).
        """
    ),
    output_key="slogan"
)

# Combinar em sequÃªncia
pipeline_marketing = SequentialChain(
    chains=[chain_ideia, chain_descricao, chain_slogan],
    input_variables=["categoria"],
    output_variables=["nome_produto", "descricao", "slogan"],
    verbose=True
)

# Executar pipeline completo
resultado = pipeline_marketing.invoke({"categoria": "fitness wearables"})

print(f"Produto: {resultado['nome_produto']}")
print(f"DescriÃ§Ã£o: {resultado['descricao']}")
print(f"Slogan: {resultado['slogan']}")</code></pre>

            <div class="info-box">
                <strong>SaÃ­da exemplo:</strong><br>
                Produto: FitPulse Pro<br>
                DescriÃ§Ã£o: O FitPulse Pro Ã© o smartwatch definitivo para atletas...<br>
                Slogan: Seu batimento, sua vitÃ³ria
            </div>

            <h3>RouterChain: DecisÃµes DinÃ¢micas</h3>
            <p>Direcione para chains diferentes baseado no input:</p>
            <pre><code>from langchain.chains.router import MultiPromptChain

# Definir chains especializadas
chains = {
    "matematica": LLMChain(...),
    "codigo": LLMChain(...),
    "geral": LLMChain(...)
}

# DescriÃ§Ãµes para o router decidir
chain_descriptions = [
    {
        "name": "matematica",
        "description": "Bom para resolver problemas matemÃ¡ticos e cÃ¡lculos"
    },
    {
        "name": "codigo",
        "description": "Bom para explicar cÃ³digo e conceitos de programaÃ§Ã£o"
    }
]

# Criar router
router_chain = MultiPromptChain.from_prompts(
    llm=llm,
    destination_chains=chains,
    destinations=chain_descriptions,
    default_chain=chains["geral"]
)

# Teste
print(router_chain.run("Qual Ã© 25 * 17?"))  # â†’ matematica
print(router_chain.run("def factorial(n): ..."))  # â†’ codigo</code></pre>

            <h3>TransformChain: Processamento de Dados</h3>
            <p>Transforme dados entre chains:</p>
            <pre><code>from langchain.chains import TransformChain

def extrair_palavras_chave(inputs: dict) -> dict:
    texto = inputs["texto"]
    # LÃ³gica customizada
    palavras = texto.lower().split()
    freq = {}
    for palavra in palavras:
        if len(palavra) > 4:
            freq[palavra] = freq.get(palavra, 0) + 1

    top_palavras = sorted(freq.items(), key=lambda x: x[1], reverse=True)[:5]
    return {"palavras_chave": [p[0] for p in top_palavras]}

transform_chain = TransformChain(
    input_variables=["texto"],
    output_variables=["palavras_chave"],
    transform=extrair_palavras_chave
)</code></pre>
        </section>

        <!-- Chapter 3.3 -->
        <section id="cap-3-3" class="content-section">
            <h2>CapÃ­tulo 3.3: RAG (Retrieval-Augmented Generation)</h2>

            <h3>O Problema do Conhecimento Limitado</h3>
            <p>LLMs tÃªm trÃªs limitaÃ§Ãµes fundamentais:</p>
            <ol>
                <li><strong>Conhecimento Datado:</strong> Treinados atÃ© uma data de corte</li>
                <li><strong>Sem Dados Privados:</strong> NÃ£o conhecem seus documentos internos</li>
                <li><strong>AlucinaÃ§Ãµes:</strong> Podem inventar informaÃ§Ãµes</li>
            </ol>

            <div class="success-box">
                <strong>RAG resolve todos esses problemas.</strong>
            </div>

            <h3>Arquitetura RAG Completa</h3>
            <div class="diagram">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  FASE 1: INDEXAÃ‡ÃƒO                      â”‚
â”‚  (Executada uma vez ou periodicamente)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Documentos â†’ Document Loader â†’ Text Splitter â†’ Embedding Model
    â†“                               â†“                â†“
  PDFs                          Chunks            Vetores
  TXTs                       (pedaÃ§os)          [0.1, 0.5,...]
  URLs                                              â†“
                                            Vector Database
                                            (Chroma, FAISS)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  FASE 2: RECUPERAÃ‡ÃƒO                    â”‚
â”‚  (Executada a cada pergunta)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Pergunta do UsuÃ¡rio â†’ Embedding Model â†’ Busca SemÃ¢ntica
      â†“                     â†“                   â†“
  "Qual garantia?"      [0.2, 0.4,...]   Top K chunks
                                         relevantes

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FASE 3: GERAÃ‡ÃƒO                       â”‚
â”‚  (Executada a cada pergunta)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Pergunta + Chunks Relevantes â†’ LLM â†’ Resposta Final
                 â†“
        Context-aware prompt</div>

            <h3>ImplementaÃ§Ã£o Completa: ChatPDF</h3>

            <h4>Passo 1: Document Loading</h4>
            <pre><code>from langchain_community.document_loaders import PyPDFLoader, TextLoader, WebBaseLoader

# Carregar PDF
loader_pdf = PyPDFLoader("manual_produto.pdf")
documentos_pdf = loader_pdf.load()

# Carregar de URL
loader_web = WebBaseLoader("https://docs.empresa.com")
documentos_web = loader_web.load()

# Carregar mÃºltiplos arquivos
from langchain_community.document_loaders import DirectoryLoader

loader = DirectoryLoader(
    "docs/",
    glob="**/*.pdf",
    loader_cls=PyPDFLoader
)
todos_docs = loader.load()

print(f"Carregados {len(todos_docs)} documentos")</code></pre>

            <h4>Passo 2: Text Splitting</h4>
            <p>Documentos precisam ser divididos em chunks para:</p>
            <ul>
                <li>Caber no contexto do LLM</li>
                <li>Melhorar precisÃ£o da busca semÃ¢ntica</li>
                <li>Otimizar performance</li>
            </ul>

            <pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # Tamanho de cada chunk
    chunk_overlap=200,      # Overlap entre chunks (continuidade)
    length_function=len,
    separators=["\n\n", "\n", " ", ""]  # Tenta quebrar em parÃ¡grafos primeiro
)

chunks = text_splitter.split_documents(todos_docs)

print(f"Documentos divididos em {len(chunks)} chunks")
print(f"Exemplo de chunk:\n{chunks[0].page_content[:200]}...")</code></pre>

            <h4>Passo 3: Embeddings</h4>
            <pre><code>from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings

# OpÃ§Ã£o 1: OpenAI (pago, alta qualidade)
embeddings_openai = OpenAIEmbeddings(
    model="text-embedding-3-small"
)

# OpÃ§Ã£o 2: Open-source (gratuito)
embeddings_hf = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# Testar embedding
vetor = embeddings_openai.embed_query("Qual Ã© a garantia?")
print(f"DimensÃµes do vetor: {len(vetor)}")
print(f"Primeiros valores: {vetor[:5]}")</code></pre>

            <h4>Passo 4: Vector Store</h4>
            <pre><code>from langchain_community.vectorstores import FAISS, Chroma

# OpÃ§Ã£o 1: FAISS (local, rÃ¡pido, sem dependÃªncias)
vectorstore_faiss = FAISS.from_documents(
    documents=chunks,
    embedding=embeddings_openai
)

# Salvar para uso posterior
vectorstore_faiss.save_local("vectorstore_local")

# Carregar
vectorstore_carregado = FAISS.load_local(
    "vectorstore_local",
    embeddings_openai,
    allow_dangerous_deserialization=True
)

# OpÃ§Ã£o 2: Chroma (persistent, com metadados)
vectorstore_chroma = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings_openai,
    persist_directory="./chroma_db"
)</code></pre>

            <h4>Passo 5: Retrieval</h4>
            <pre><code># Busca simples por similaridade
query = "Qual Ã© a polÃ­tica de devoluÃ§Ã£o?"
docs_relevantes = vectorstore_faiss.similarity_search(
    query,
    k=4  # Top 4 resultados mais relevantes
)

for i, doc in enumerate(docs_relevantes):
    print(f"\n--- Documento {i+1} ---")
    print(doc.page_content[:200])
    print(f"Metadados: {doc.metadata}")

# Busca com score de similaridade
docs_com_score = vectorstore_faiss.similarity_search_with_score(
    query,
    k=4
)

for doc, score in docs_com_score:
    print(f"Score: {score:.4f} | {doc.page_content[:100]}...")</code></pre>

            <h4>Passo 6: GeraÃ§Ã£o com Contexto</h4>
            <pre><code>from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4", temperature=0)

# Criar chain RAG completa
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # Como combinar documentos
    retriever=vectorstore_faiss.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=True,  # Retornar fontes
    verbose=True
)

# Fazer pergunta
pergunta = "Qual Ã© o perÃ­odo de garantia do produto X?"
resultado = qa_chain.invoke({"query": pergunta})

print(f"Pergunta: {pergunta}")
print(f"\nResposta: {resultado['result']}")
print(f"\nFontes utilizadas:")
for doc in resultado['source_documents']:
    print(f"- {doc.metadata['source']}, pÃ¡gina {doc.metadata.get('page', 'N/A')}")</code></pre>

            <div class="info-box">
                <strong>Chain Types explicados:</strong><br>
                1. <strong>"stuff"</strong> - Coloca tudo em um prompt (melhor para poucos docs)<br>
                2. <strong>"map_reduce"</strong> - Processa docs em paralelo, depois combina<br>
                3. <strong>"refine"</strong> - Processa docs sequencialmente, refinando resposta<br>
                4. <strong>"map_rerank"</strong> - Pontua cada doc, escolhe melhor resposta
            </div>

            <h3>RAG Conversacional (com memÃ³ria)</h3>
            <pre><code>from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# Criar memÃ³ria
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"
)

# Chain conversacional
conversational_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore_faiss.as_retriever(),
    memory=memory,
    return_source_documents=True
)

# Conversa multi-turn
perguntas = [
    "Qual Ã© a garantia do produto?",
    "E se eu quiser estender ela?",  # Usa contexto anterior
    "Quanto custa?"  # Ainda referencia "garantia estendida"
]

for pergunta in perguntas:
    resultado = conversational_chain.invoke({"question": pergunta})
    print(f"\nQ: {pergunta}")
    print(f"A: {resultado['answer']}")</code></pre>
        </section>

        <!-- Chapter 3.4 -->
        <section id="cap-3-4" class="content-section">
            <h2>CapÃ­tulo 3.4: Agentes LangChain - InteligÃªncia AutÃ´noma</h2>

            <h3>Do Scripted ao AutÃ´nomo</h3>
            <p><strong>Chains</strong> sÃ£o como seguir uma receita:</p>
            <ul>
                <li>Passo 1: FaÃ§a X</li>
                <li>Passo 2: FaÃ§a Y</li>
                <li>Passo 3: FaÃ§a Z</li>
            </ul>

            <p><strong>Agents</strong> sÃ£o como ter um chef que decide:</p>
            <ul>
                <li>Qual receita seguir</li>
                <li>Que ingredientes usar</li>
                <li>Quando improvisar</li>
            </ul>

            <h3>O PadrÃ£o ReAct (Reason + Act)</h3>
            <div class="diagram">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CICLO DO AGENTE REACT               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. THOUGHT (Pensamento)
   "Preciso saber o clima atual em SÃ£o Paulo"
   â†“
2. ACTION (AÃ§Ã£o)
   Tool: weather_api
   â†“
3. ACTION INPUT (Entrada)
   Input: "SÃ£o Paulo, Brasil"
   â†“
4. OBSERVATION (ObservaÃ§Ã£o)
   Result: "25Â°C, ensolarado"
   â†“
5. THOUGHT (Pensamento)
   "Agora tenho a informaÃ§Ã£o, posso responder"
   â†“
6. FINAL ANSWER (Resposta Final)
   "O clima em SÃ£o Paulo estÃ¡ 25Â°C e ensolarado"</div>

            <h3>Criando Seu Primeiro Agente</h3>

            <h4>Passo 1: Definir Tools</h4>
            <pre><code>from langchain.tools import Tool
from langchain_community.utilities import WikipediaAPIWrapper
import requests

# Tool 1: Wikipedia
wikipedia = WikipediaAPIWrapper()
tool_wikipedia = Tool(
    name="Wikipedia",
    func=wikipedia.run,
    description="""
    Ãštil para buscar informaÃ§Ãµes factuais sobre pessoas, lugares,
    eventos histÃ³ricos, etc. Input deve ser uma pergunta ou termo
    de busca em portuguÃªs.
    """
)

# Tool 2: Calculadora
def calculadora(expression: str) -> str:
    """Avalia expressÃµes matemÃ¡ticas"""
    try:
        return str(eval(expression))
    except Exception as e:
        return f"Erro: {str(e)}"

tool_calc = Tool(
    name="Calculadora",
    func=calculadora,
    description="""
    Ãštil para fazer cÃ¡lculos matemÃ¡ticos. Input deve ser uma
    expressÃ£o matemÃ¡tica vÃ¡lida em Python (ex: '2 + 2', '10 ** 3').
    """
)

# Tool 3: Busca na Web (exemplo com DuckDuckGo)
from langchain_community.tools import DuckDuckGoSearchRun

search = DuckDuckGoSearchRun()
tool_search = Tool(
    name="WebSearch",
    func=search.run,
    description="""
    Ãštil para buscar informaÃ§Ãµes atualizadas na internet.
    Use quando precisar de notÃ­cias recentes, dados atuais,
    ou informaÃ§Ãµes que nÃ£o estÃ£o na Wikipedia.
    """
)

tools = [tool_wikipedia, tool_calc, tool_search]</code></pre>

            <h4>Passo 2: Criar o Agente</h4>
            <pre><code>from langchain.agents import create_react_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate

llm = ChatOpenAI(model="gpt-4", temperature=0)

# Prompt do agente ReAct
react_prompt = PromptTemplate.from_template("""
Responda Ã  seguinte pergunta o melhor que puder. VocÃª tem acesso Ã s seguintes ferramentas:

{tools}

Use o seguinte formato:

Question: a pergunta de entrada que vocÃª deve responder
Thought: vocÃª deve sempre pensar sobre o que fazer
Action: a aÃ§Ã£o a tomar, deve ser uma de [{tool_names}]
Action Input: a entrada para a aÃ§Ã£o
Observation: o resultado da aÃ§Ã£o
... (este Thought/Action/Action Input/Observation pode se repetir N vezes)
Thought: Agora eu sei a resposta final
Final Answer: a resposta final para a pergunta original

Comece!

Question: {input}
Thought: {agent_scratchpad}
""")

# Criar agente
agent = create_react_agent(
    llm=llm,
    tools=tools,
    prompt=react_prompt
)

# Executor (gerencia execuÃ§Ã£o)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True,  # Mostra raciocÃ­nio
    max_iterations=10,  # Limite de seguranÃ§a
    handle_parsing_errors=True
)</code></pre>

            <h4>Passo 3: Executar o Agente</h4>
            <pre><code># Teste 1: Requer Wikipedia
resposta1 = agent_executor.invoke({
    "input": "Quem foi Albert Einstein e em que ano nasceu?"
})
print(resposta1['output'])

# Teste 2: Requer Calculadora
resposta2 = agent_executor.invoke({
    "input": "Qual Ã© a raiz quadrada de 12345?"
})
print(resposta2['output'])

# Teste 3: Requer mÃºltiplas tools
resposta3 = agent_executor.invoke({
    "input": """
    O PIB do Brasil em 2023 foi de 10.9 trilhÃµes de reais.
    Se dividirmos isso pela populaÃ§Ã£o de 215 milhÃµes,
    quanto Ã© o PIB per capita em reais?
    """
})
print(resposta3['output'])
# Agente vai: buscar confirmaÃ§Ã£o do PIB â†’ calcular divisÃ£o â†’ responder</code></pre>

            <div class="info-box">
                <strong>Exemplo de saÃ­da (verbose=True):</strong><br>
                <pre>> Entering new AgentExecutor chain...

Question: Qual Ã© a raiz quadrada de 12345?

Thought: Preciso calcular a raiz quadrada de um nÃºmero.
Action: Calculadora
Action Input: 12345 ** 0.5
Observation: 111.1081249402829

Thought: Agora eu sei a resposta final.
Final Answer: A raiz quadrada de 12345 Ã© aproximadamente 111.11.

> Finished chain.</pre>
            </div>

            <h3>Tipos de Agentes no LangChain</h3>

            <h4>1. Zero-Shot ReAct Agent</h4>
            <pre><code>from langchain.agents import create_react_agent

# Decide ferramentas baseado apenas em descriÃ§Ãµes
agent_zero_shot = create_react_agent(
    llm=llm,
    tools=tools,
    prompt=react_prompt
)</code></pre>

            <h4>2. Conversational Agent (com memÃ³ria)</h4>
            <pre><code>from langchain.agents import create_conversational_agent
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

conversational_agent = create_conversational_agent(
    llm=llm,
    tools=tools,
    memory=memory,
    verbose=True
)

# Agora mantÃ©m contexto entre perguntas
executor = AgentExecutor(agent=conversational_agent, tools=tools)

executor.invoke({"input": "Pesquise sobre Python"})
executor.invoke({"input": "E quanto a performance?"})  # Lembra que Ã© sobre Python</code></pre>

            <h4>3. OpenAI Functions Agent</h4>
            <pre><code>from langchain.agents import create_openai_functions_agent

# Usa function calling nativo do GPT-4
functions_agent = create_openai_functions_agent(
    llm=llm,
    tools=tools,
    prompt=react_prompt
)

# Mais confiÃ¡vel e menos propenso a erros de parsing</code></pre>

            <h3>Criando Tools Customizadas</h3>

            <h4>Tool Simples (funÃ§Ã£o)</h4>
            <pre><code>@tool
def consultar_estoque(produto: str) -> str:
    """
    Consulta estoque de um produto no sistema.
    Args:
        produto: Nome do produto a consultar
    """
    # Simular consulta ao banco
    estoque_db = {
        "notebook": 15,
        "mouse": 50,
        "teclado": 30
    }
    quantidade = estoque_db.get(produto.lower(), 0)
    return f"Estoque de {produto}: {quantidade} unidades"

tools.append(consultar_estoque)</code></pre>

            <h4>Tool com API Externa</h4>
            <pre><code>from langchain.tools import tool
import requests

@tool
def buscar_cep(cep: str) -> str:
    """
    Busca endereÃ§o a partir do CEP brasileiro.
    Args:
        cep: CEP no formato 12345-678 ou 12345678
    """
    cep_limpo = cep.replace("-", "")
    response = requests.get(f"https://viacep.com.br/ws/{cep_limpo}/json/")

    if response.status_code == 200:
        data = response.json()
        if "erro" not in data:
            return f"""
            EndereÃ§o:
            {data['logradouro']}, {data['bairro']}
            {data['localidade']} - {data['uf']}
            """
    return "CEP nÃ£o encontrado"</code></pre>
        </section>

        <!-- Chapter 3.5 -->
        <section id="cap-3-5" class="content-section">
            <h2>CapÃ­tulo 3.5: Memory - Dando MemÃ³ria aos Agentes</h2>

            <h3>Por Que MemÃ³ria Ã© Crucial</h3>
            <p>LLMs sÃ£o <strong>stateless</strong> por padrÃ£o - cada chamada Ã© independente. MemÃ³ria permite:</p>
            <ul>
                <li>Conversas naturais multi-turn</li>
                <li>PersonalizaÃ§Ã£o baseada em histÃ³rico</li>
                <li>Contexto acumulado</li>
                <li>Aprendizado de preferÃªncias</li>
            </ul>

            <h3>Tipos de Memory no LangChain</h3>

            <h4>1. ConversationBufferMemory (MemÃ³ria Completa)</h4>
            <pre><code>from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory()

# Salvar interaÃ§Ãµes
memory.save_context(
    {"input": "OlÃ¡! Meu nome Ã© Ana"},
    {"output": "OlÃ¡ Ana! Como posso ajudar?"}
)

memory.save_context(
    {"input": "Preciso de ajuda com Python"},
    {"output": "Claro! Qual sua dÃºvida sobre Python?"}
)

# Recuperar histÃ³rico completo
print(memory.load_memory_variables({}))</code></pre>

            <div class="info-box">
                <strong>SaÃ­da:</strong><br>
                <pre>{
  'history': 'Human: OlÃ¡! Meu nome Ã© Ana\nAI: OlÃ¡ Ana! Como posso ajudar?\nHuman: Preciso de ajuda com Python\nAI: Claro! Qual sua dÃºvida sobre Python?'
}</pre>
            </div>

            <p><strong>Usar com Chain:</strong></p>
            <pre><code>from langchain.chains import ConversationChain

conversation = ConversationChain(
    llm=llm,
    memory=ConversationBufferMemory(),
    verbose=True
)

conversation.predict(input="Oi, sou JoÃ£o")
conversation.predict(input="Qual Ã© meu nome?")  # Vai lembrar: JoÃ£o</code></pre>

            <h4>2. ConversationBufferWindowMemory (Janela Deslizante)</h4>
            <pre><code>from langchain.memory import ConversationBufferWindowMemory

# MantÃ©m apenas Ãºltimas K interaÃ§Ãµes
memory_window = ConversationBufferWindowMemory(k=3)

# Depois de 10 interaÃ§Ãµes, sÃ³ lembra as 3 Ãºltimas</code></pre>

            <p><strong>Quando usar:</strong></p>
            <ul>
                <li>Conversas longas</li>
                <li>Limitar custos de tokens</li>
                <li>Focar em contexto recente</li>
            </ul>

            <h4>3. ConversationSummaryMemory (Resumo Inteligente)</h4>
            <pre><code>from langchain.memory import ConversationSummaryMemory

# Resume conversas longas
memory_summary = ConversationSummaryMemory(llm=llm)

memory_summary.save_context(
    {"input": "Quero comprar um notebook para programaÃ§Ã£o"},
    {"output": "Recomendo um com pelo menos 16GB RAM e SSD"}
)

memory_summary.save_context(
    {"input": "Qual processador vocÃª recomenda?"},
    {"output": "Intel i7 ou AMD Ryzen 7 sÃ£o Ã³timas opÃ§Ãµes"}
)

# Em vez de guardar tudo, cria resumo
print(memory_summary.load_memory_variables({}))</code></pre>

            <div class="info-box">
                <strong>SaÃ­da:</strong><br>
                <pre>{
  'history': 'O usuÃ¡rio estÃ¡ interessado em comprar um notebook para programaÃ§Ã£o. Recomendei especificaÃ§Ãµes mÃ­nimas de 16GB RAM, SSD, e processadores Intel i7 ou AMD Ryzen 7.'
}</pre>
            </div>

            <h4>4. ConversationSummaryBufferMemory (HÃ­brido)</h4>
            <pre><code>from langchain.memory import ConversationSummaryBufferMemory

# Resume antigas, mantÃ©m recentes completas
memory_hybrid = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=500  # Limite antes de resumir
)</code></pre>

            <h4>5. ConversationKGMemory (Knowledge Graph)</h4>
            <pre><code>from langchain.memory import ConversationKGMemory

# Extrai relaÃ§Ãµes entre entidades
memory_kg = ConversationKGMemory(llm=llm)

memory_kg.save_context(
    {"input": "JoÃ£o mora em SÃ£o Paulo"},
    {"output": "Entendi"}
)

memory_kg.save_context(
    {"input": "Ele trabalha na Google"},
    {"output": "Anotado"}
)

# Cria grafo: JoÃ£o --mora_em--> SÃ£o Paulo
#             JoÃ£o --trabalha_em--> Google</code></pre>

            <h3>Memory Persistente (Banco de Dados)</h3>
            <pre><code>from langchain.memory import ConversationBufferMemory
from langchain.memory.chat_message_histories import RedisChatMessageHistory

# Salvar em Redis
message_history = RedisChatMessageHistory(
    url="redis://localhost:6379/0",
    session_id="user_123"
)

memory_redis = ConversationBufferMemory(
    chat_memory=message_history
)

# Ou SQLite
from langchain.memory.chat_message_histories import SQLChatMessageHistory

sql_history = SQLChatMessageHistory(
    session_id="user_123",
    connection_string="sqlite:///chat_history.db"
)

memory_sql = ConversationBufferMemory(chat_memory=sql_history)</code></pre>

            <h3>Memory com Agentes</h3>
            <pre><code>from langchain.agents import initialize_agent, AgentType
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

agent_with_memory = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,
    memory=memory,
    verbose=True
)

# Conversa com contexto
agent_with_memory.invoke({"input": "Meu nome Ã© Carlos e gosto de futebol"})
agent_with_memory.invoke({"input": "Qual meu esporte favorito?"})  # Lembra!
agent_with_memory.invoke({"input": "E meu nome?"})  # TambÃ©m lembra!</code></pre>
        </section>

        <!-- Chapter 3.6 -->
        <section id="cap-3-6" class="content-section">
            <h2>CapÃ­tulo 3.6: IntegraÃ§Ãµes e Ecosystem</h2>

            <h3>Document Loaders (60+ tipos)</h3>
            <pre><code># PDFs
from langchain_community.document_loaders import PyPDFLoader, PDFMinerLoader

# Web
from langchain_community.document_loaders import WebBaseLoader, SeleniumURLLoader

# Office
from langchain_community.document_loaders import UnstructuredWordDocumentLoader, UnstructuredExcelLoader

# CÃ³digo
from langchain_community.document_loaders import GitLoader, NotebookLoader

# Databases
from langchain_community.document_loaders import DataFrameLoader, SQLDatabaseLoader

# APIs
from langchain_community.document_loaders import GoogleDriveLoader, NotionDBLoader</code></pre>

            <h3>Vector Stores (50+ opÃ§Ãµes)</h3>
            <pre><code># Local
from langchain_community.vectorstores import FAISS, Chroma, DocArrayInMemorySearch

# Cloud
from langchain_community.vectorstores import Pinecone, Weaviate, Qdrant

# Databases
from langchain_community.vectorstores import PGVector, Redis, ElasticsearchStore</code></pre>

            <h3>LLM Providers</h3>
            <pre><code># ProprietÃ¡rios
from langchain_openai import ChatOpenAI, OpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI

# Open-source
from langchain_community.llms import Ollama, HuggingFaceHub, LlamaCpp

# Self-hosted
from langchain_community.llms import HuggingFacePipeline</code></pre>
        </section>

        <!-- Summary -->
        <section class="content-section">
            <h2>ğŸ“ Resumo GrÃ¡fico do MÃ³dulo 3</h2>

            <h3>Conceitos-Chave</h3>
            <p><strong>LangChain = Framework de OrquestraÃ§Ã£o</strong></p>
            <ul>
                <li>Modularidade + Composabilidade</li>
                <li>AbstraÃ§Ã£o de complexidade</li>
                <li>Ecosystem rico</li>
            </ul>

            <p><strong>6 Pilares:</strong></p>
            <ol>
                <li>Models (LLMs)</li>
                <li>Prompts (Templates)</li>
                <li>Chains (Pipelines)</li>
                <li>Agents (Autonomia)</li>
                <li>Memory (Contexto)</li>
                <li>Tools (ConexÃµes externas)</li>
            </ol>

            <p><strong>RAG (Retrieval-Augmented Generation):</strong></p>
            <ul>
                <li>IndexaÃ§Ã£o â†’ Embedding â†’ VectorDB</li>
                <li>Retrieval â†’ Busca semÃ¢ntica</li>
                <li>Generation â†’ LLM com contexto</li>
            </ul>

            <p><strong>Agents:</strong></p>
            <ul>
                <li>ReAct (Reason + Act)</li>
                <li>Tools customizadas</li>
                <li>DecisÃµes autÃ´nomas</li>
            </ul>
        </section>

        <!-- Project -->
        <section id="projeto" class="project-section">
            <h2>ğŸš€ Projeto PrÃ¡tico do MÃ³dulo 3</h2>

            <h3>Desafio: ChatPDF Completo com Interface Web</h3>

            <p><strong>Objetivo:</strong> Construir uma aplicaÃ§Ã£o completa de RAG que permite:</p>
            <ol>
                <li>Upload de mÃºltiplos PDFs</li>
                <li>Processamento e indexaÃ§Ã£o automÃ¡tica</li>
                <li>Chat conversacional com memÃ³ria</li>
                <li>CitaÃ§Ã£o de fontes</li>
                <li>Interface web intuitiva</li>
            </ol>

            <h4>EspecificaÃ§Ãµes TÃ©cnicas</h4>
            <p><strong>Stack:</strong></p>
            <ul>
                <li>LangChain para RAG</li>
                <li>OpenAI para embeddings e LLM</li>
                <li>FAISS para vector store (local)</li>
                <li>Streamlit para interface web</li>
                <li>Python 3.10+</li>
            </ul>

            <div class="info-box">
                <strong>ğŸ“ CÃ³digo completo disponÃ­vel:</strong><br>
                Veja o arquivo completo do projeto ChatPDF (200+ linhas) no conteÃºdo markdown do mÃ³dulo, incluindo:<br>
                â€¢ Interface Streamlit completa<br>
                â€¢ Sistema de upload e processamento de PDFs<br>
                â€¢ RAG conversacional com memÃ³ria<br>
                â€¢ CitaÃ§Ã£o de fontes com metadados<br>
                â€¢ requirements.txt e instruÃ§Ãµes de setup
            </div>

            <h4>Como Executar</h4>
            <pre><code># 1. Criar ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# 2. Instalar dependÃªncias
pip install -r requirements.txt

# 3. Executar aplicaÃ§Ã£o
streamlit run chatpdf_app.py</code></pre>
        </section>

        <!-- Tips -->
        <section id="dicas" class="content-section">
            <h2>ğŸ’¡ Dicas de OtimizaÃ§Ã£o e Debugging</h2>

            <h3>1. Debugging de Chains</h3>
            <pre><code># Ativar verbose
chain = LLMChain(llm=llm, prompt=prompt, verbose=True)

# Callbacks customizados
from langchain.callbacks import StdOutCallbackHandler

handler = StdOutCallbackHandler()
chain.run(input="...", callbacks=[handler])

# LangSmith (plataforma oficial)
import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "..."</code></pre>

            <h3>2. Otimizando RAG</h3>
            <pre><code># Cache de embeddings
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore

store = LocalFileStore("./cache/")
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
    underlying_embeddings,
    store,
    namespace="openai_embeddings"
)

# CompressÃ£o de contexto
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

compressor = LLMChainExtractor.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever
)</code></pre>

            <h3>3. Gerenciamento de Custos</h3>
            <pre><code>from langchain.callbacks import get_openai_callback

with get_openai_callback() as cb:
    result = chain.run(input="...")
    print(f"Tokens: {cb.total_tokens}")
    print(f"Custo: ${cb.total_cost}")</code></pre>
        </section>

        <!-- Next Steps -->
        <section class="content-section">
            <h2>ğŸ¯ PrÃ³ximos Passos</h2>

            <p>VocÃª dominou LangChain! Agora sabe como:</p>
            <ul>
                <li>âœ“ Criar chains complexas</li>
                <li>âœ“ Implementar RAG completo</li>
                <li>âœ“ Construir agentes autÃ´nomos</li>
                <li>âœ“ Gerenciar memÃ³ria conversacional</li>
                <li>âœ“ Integrar ferramentas externas</li>
                <li>âœ“ Deploy de aplicaÃ§Ãµes reais</li>
            </ul>

            <div class="info-box">
                <strong>No MÃ³dulo 4</strong>, exploraremos o <strong>Agno</strong>, um framework mais recente e minimalista que oferece uma perspectiva diferente sobre a construÃ§Ã£o de agentes autÃ´nomos, focando na simplicidade e elegÃ¢ncia.
            </div>
        </section>

        <!-- Navigation -->
        <nav class="module-navigation">
            <a href="modulo-02.html" class="nav-button prev">
                â† MÃ³dulo 2: Engenharia de Prompts
            </a>
            <a href="modulo-04.html" class="nav-button next">
                MÃ³dulo 4: Agno â†’
            </a>
        </nav>

    </main>

    <!-- Footer -->
    <footer style="background: #1e293b; color: white; padding: 3rem 0; margin-top: 4rem;">
        <div class="container" style="text-align: center;">
            <p style="margin-bottom: 1rem;">
                <strong>FEA-IA 2.0</strong> - FormaÃ§Ã£o Completa em Engenharia de Agentes IA
            </p>
            <p style="font-size: 0.875rem; opacity: 0.8;">
                ğŸ“„ 1.801 linhas | 18.000+ palavras | NÃ­vel AplicaÃ§Ã£o
            </p>
            <p style="font-size: 0.875rem; opacity: 0.6; margin-top: 1rem;">
                Desenvolvido com ğŸ’œ para a comunidade de IA
            </p>
        </div>
    </footer>

    <script>
        // Topics toggle functionality
        const toggleButton = document.getElementById('topics-toggle');
        const topicsList = document.getElementById('topics-list');
        const topicsContainer = document.getElementById('topics-container');
        const toggleIcon = toggleButton.querySelector('.toggle-icon');

        toggleButton.addEventListener('click', () => {
            topicsContainer.classList.toggle('collapsed');
            toggleIcon.textContent = topicsContainer.classList.contains('collapsed') ? 'â–¶' : 'â–¼';
        });

        // Smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });

        // Highlight active section in topics
        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    const id = entry.target.getAttribute('id');
                    document.querySelectorAll('.topic-item').forEach(item => {
                        item.classList.remove('active');
                        if (item.getAttribute('href') === `#${id}`) {
                            item.classList.add('active');
                        }
                    });
                }
            });
        }, {
            rootMargin: '-100px 0px -66%'
        });

        document.querySelectorAll('section[id]').forEach(section => {
            observer.observe(section);
        });
    </script>
</body>
</html>
